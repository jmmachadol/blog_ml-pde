<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Perceptrón Multicapa: una mirada a las redes neuronales - Machine Learning &amp; Partial Differential Equations</title><meta name="description" content="Introducción al Perceptrón Multicapa (MLP) En el mundo de las redes neuronales, el perceptrón multicapa (MLP) tiene un lugar prominente. Es una arquitectura de red que ha sido fundamental para el avance de los algoritmos de aprendizaje profundo. Un MLP consiste en una serie de&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://jmmachadol.github.io/blog_ml-pde/perceptron-multicapa-una-mirada-a-las-redes-neuronales.html"><link rel="alternate" type="application/atom+xml" href="https://jmmachadol.github.io/blog_ml-pde/feed.xml"><link rel="alternate" type="application/json" href="https://jmmachadol.github.io/blog_ml-pde/feed.json"><meta property="og:title" content="Perceptrón Multicapa: una mirada a las redes neuronales"><meta property="og:site_name" content="Machine Learning & Partial Differential Equations"><meta property="og:description" content="Introducción al Perceptrón Multicapa (MLP) En el mundo de las redes neuronales, el perceptrón multicapa (MLP) tiene un lugar prominente. Es una arquitectura de red que ha sido fundamental para el avance de los algoritmos de aprendizaje profundo. Un MLP consiste en una serie de&hellip;"><meta property="og:url" content="https://jmmachadol.github.io/blog_ml-pde/perceptron-multicapa-una-mirada-a-las-redes-neuronales.html"><meta property="og:type" content="article"><link rel="stylesheet" href="https://jmmachadol.github.io/blog_ml-pde/assets/css/style.css?v=ad994e06fe4ccf9aa9bbf0f25d02eaf2"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://jmmachadol.github.io/blog_ml-pde/perceptron-multicapa-una-mirada-a-las-redes-neuronales.html"},"headline":"Perceptrón Multicapa: una mirada a las redes neuronales","datePublished":"2023-06-28T17:48","dateModified":"2023-06-28T18:38","description":"Introducción al Perceptrón Multicapa (MLP) En el mundo de las redes neuronales, el perceptrón multicapa (MLP) tiene un lugar prominente. Es una arquitectura de red que ha sido fundamental para el avance de los algoritmos de aprendizaje profundo. Un MLP consiste en una serie de&hellip;","author":{"@type":"Person","name":"Jose Manuel Machado-Loaiza","url":"https://jmmachadol.github.io/blog_ml-pde/authors/jose-manuel-machado-loaiza/"},"publisher":{"@type":"Organization","name":"Jose Manuel Machado-Loaiza"}}</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://jmmachadol.github.io/blog_ml-pde/">Machine Learning &amp; Partial Differential Equations</a></header><main><article class="post"><div class="hero"><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2023-06-28T17:48">June 28, 2023</time></div><h1>Perceptrón Multicapa: una mirada a las redes neuronales</h1><div class="post__meta post__meta--author"><a href="https://jmmachadol.github.io/blog_ml-pde/authors/jose-manuel-machado-loaiza/" class="feed__author">Jose Manuel Machado-Loaiza</a></div></div></header></div><div class="wrapper post__entry"><h2>Introducción al Perceptrón Multicapa (MLP)</h2><p>En el mundo de las redes neuronales, el perceptrón multicapa (MLP) tiene un lugar prominente. Es una arquitectura de red que ha sido fundamental para el avance de los algoritmos de aprendizaje profundo. Un MLP consiste en una serie de capas de nodos (o "neuronas"), donde cada capa se conecta a la siguiente, creando una estructura en cascada. Estas redes pueden manejar patrones complejos y no lineales, lo que las hace extremadamente útiles en una variedad de aplicaciones.</p><figure><figure class="post__image"><img loading="lazy" style="width: 806px;" src="https://jmmachadol.github.io/blog_ml-pde/media/posts/7/mlp_regressor-3.jpg" alt="" height="387" sizes="100vw" srcset="https://jmmachadol.github.io/blog_ml-pde/media/posts/7/responsive/mlp_regressor-3-xs.jpg 300w, https://jmmachadol.github.io/blog_ml-pde/media/posts/7/responsive/mlp_regressor-3-sm.jpg 480w, https://jmmachadol.github.io/blog_ml-pde/media/posts/7/responsive/mlp_regressor-3-md.jpg 768w, https://jmmachadol.github.io/blog_ml-pde/media/posts/7/responsive/mlp_regressor-3-lg.jpg 1024w, https://jmmachadol.github.io/blog_ml-pde/media/posts/7/responsive/mlp_regressor-3-xl.jpg 1360w, https://jmmachadol.github.io/blog_ml-pde/media/posts/7/responsive/mlp_regressor-3-2xl.jpg 1600w"></figure><figcaption>Figura 1: Esquema de un perceptrón multicapa con dos capas ocultas (Fuente: Haykin, 2009).</figcaption></figure><h2>Características del MLP</h2><p>Existen varias características clave que definen un MLP (Goodfellow et al., 2016; Rumelhart et al., 1986; Haykin, 2009):</p><ul><li>Cada neurona en la red utiliza una función de activación que es diferenciable. Si bien las funciones de activación no lineales son comunes, también se pueden usar funciones de activación lineales en ciertos casos.</li><li>La red consiste en una capa de entrada, una o más capas ocultas, y una capa de salida. Las capas ocultas son aquellas que no están directamente conectadas con las entradas ni las salidas de la red.</li><li>La red muestra un alto grado de conectividad, determinado por los pesos sinápticos de la red. Las neuronas pueden conectarse de muchas formas diferentes para crear tipos especiales de redes (por ejemplo, redes convolucionales o recurrentes). En esta entrada, consideraremos que las redes están completamente conectadas, a menos que se indique lo contrario.</li></ul><h2>¿Qué es un Perceptrón?</h2><p>Un perceptrón se puede definir como una neurona artificial que toma un conjunto de entradas y produce una única salida a través de un proceso que implica la suma ponderada de las entradas, un sesgo y una función de activación. Aunque las funciones de activación no lineales diferenciables son comunes, también se pueden emplear funciones de activación lineales en ciertos casos. Vale la pena mencionar que esta definición se refiere al comportamiento de una única neurona en la red neuronal, y en el caso de una red neuronal con múltiples salidas, la salida final pertenecerá a R<sup>m</sup>. Un perceptrón logra esto aplicando tres pasos fundamentales a las entradas (Goodfellow et al., 2016):</p><ol><li>Suma el producto de cada entrada con un peso correspondiente. Cada peso se puede interpretar como una medida de cuán sensible es un perceptrón a cada entrada individual.</li><li>Añade un sesgo a esta suma. Este sesgo se puede interpretar como una medida de cuán activa sería una neurona si todas las entradas fueran cero.</li><li>Pasa este valor a través de una función de activación.</li></ol><p>Por lo tanto, la activación de un perceptrón se puede representar como:</p><p><strong>a = σ(Σ(x<sub>i</sub> * w<sub>i</sub>) + β)</strong></p><p>donde <strong>a</strong> es la activación, <strong>σ</strong> es la función de activación, <strong>x<sub>i</sub></strong> son las entradas, <strong>w<sub>i</sub></strong> son los pesos, y <strong>β</strong> es el sesgo.</p><h2>Ejemplo de Regresión con MLP</h2><p>Para entender mejor cómo se puede utilizar un perceptrón multicapa, veamos un ejemplo sencillo de un problema de regresión. Utilizaremos la biblioteca scikit-learn de Python, que proporciona una implementación eficiente de MLP para la regresión (MLPRegressor).</p><pre><code>
            from sklearn.neural_network import MLPRegressor
            from sklearn.datasets import make_regression
            from sklearn.model_selection import train_test_split

            # Generamos un dataset para regresión
            X, y = make_regression(n_samples=200, random_state=1)

            # Dividimos el dataset en conjunto de entrenamiento y prueba
            X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)

            # Creamos el modelo de MLP
            regr = MLPRegressor(random_state=1, max_iter=500).fit(X_train, y_train)

            # Evaluamos el modelo en el conjunto de prueba
            score = regr.score(X_test, y_test)
            print('Score:', score)
        </code></pre><h2>Conclusión</h2><p>En resumen, el perceptrón multicapa es una herramienta potente y flexible en el campo de las redes neuronales y el aprendizaje profundo. Con sus múltiples capas de neuronas interconectadas, es capaz de aprender y representar patrones complejos y no lineales. Aunque hay mucho más que se puede explorar en relación con los MLPs, esta entrada proporciona una introducción sólida a sus conceptos fundamentales.</p><h2>Referencias</h2><ul><li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.</li><li>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533–536.</li><li>Haykin, S. (2009). Neural Networks and Learning Machines (3rd Edition). Pearson.</li><li>scikit-learn developers (2022). MLPRegressor. Retrieved from: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html">https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html</a></li></ul></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on June 28, 2023</p><div class="post__share"></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://jmmachadol.github.io/blog_ml-pde/authors/jose-manuel-machado-loaiza/" rel="author">Jose Manuel Machado-Loaiza</a></h3></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://jmmachadol.github.io/blog_ml-pde/assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="https://jmmachadol.github.io/blog_ml-pde/parametros-e-hiperparametros-de-las-redes-neuronales-que-son-y-cuales-son-sus-diferencias.html" class="post__nav-link" rel="prev"><span>Previous</span> Parámetros e hiperparámetros de las redes neuronales</a></div><div class="post__nav-next"><a href="https://jmmachadol.github.io/blog_ml-pde/oe1.html" class="post__nav-link" rel="next"><span>Next</span> Teorema de Aproximación y Retropropagación </a><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://jmmachadol.github.io/blog_ml-pde/assets/svg/svg-map.svg#arrow-next"/></svg></div></div></nav><div class="post__related related"><div class="wrapper"><h2 class="h5 related__title">You should also read:</h2><article class="related__item"><div class="feed__meta"><time datetime="2023-06-23T17:14" class="feed__date">June 23, 2023</time></div><h3 class="h1"><a href="https://jmmachadol.github.io/blog_ml-pde/parametros-e-hiperparametros-de-las-redes-neuronales-que-son-y-cuales-son-sus-diferencias.html">Parámetros e hiperparámetros de las redes neuronales</a></h3></article></div></div><div class="post__comments"><div class="wrapper"><div class="comments"><div class="comments-wrapper"><h2>Comments</h2><div id="disqus_thread"></div><noscript>Please enable JS to use the comments form.</noscript><script type="text/javascript">var disqus_config = function () {
							this.page.url = 'https://jmmachadol.github.io/blog_ml-pde/perceptron-multicapa-una-mirada-a-las-redes-neuronales.html';
							this.page.identifier = '7'; 
							this.language = 'en_GB';
						};
						
						
				var disqus_element_to_check = document.getElementById('disqus_thread');

				if ('IntersectionObserver' in window) {
					var iObserver = new IntersectionObserver(
						(entries, observer) => {
							entries.forEach(entry => {
								if (entry.intersectionRatio >= 0.1) {
									(function () {
										var d = document, s = d.createElement('script');
										s.src = 'https://'+('https-jmmachadol-github-io-blog-ml-pde').trim()+'.disqus.com/embed.js';
										s.setAttribute('data-timestamp', +new Date());
										(d.head || d.body).appendChild(s);
									})();
									observer.unobserve(entry.target);
								}
							});
						},
						{
							threshold: [0, 0.2, 0.5, 1]
						}
					);

					iObserver.observe(disqus_element_to_check);
				} else {
					(function () {
						var d = document, s = d.createElement('script');
						s.src = 'https://'+('https-jmmachadol-github-io-blog-ml-pde').trim()+'.disqus.com/embed.js';
						s.setAttribute('data-timestamp', +new Date());
						(d.head || d.body).appendChild(s);
					})();
				}</script><script type="text/javascript"></script></div></div></div></div></main><footer class="footer"><div class="footer__copyright"><p>Powered by Publii</p></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg><use xlink:href="https://jmmachadol.github.io/blog_ml-pde/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script defer="defer" src="https://jmmachadol.github.io/blog_ml-pde/assets/js/scripts.min.js?v=f47c11534595205f20935f0db2a62a85"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>