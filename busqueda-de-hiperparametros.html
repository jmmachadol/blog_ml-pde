<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Búsqueda de Hiperparámetros - Machine Learning &amp; Partial Differential Equations</title><meta name="description" content="Introducción La optimización de hiperparámetros es un componente esencial del entrenamiento de modelos de aprendizaje automático, como las redes neuronales multicapa (MLP). Los hiperparámetros son configuraciones del modelo que deben ser definidas antes del entrenamiento, y pueden influir significativamente en la capacidad del modelo para&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://jmmachadol.github.io/blog_ml-pde/busqueda-de-hiperparametros.html"><link rel="alternate" type="application/atom+xml" href="https://jmmachadol.github.io/blog_ml-pde/feed.xml"><link rel="alternate" type="application/json" href="https://jmmachadol.github.io/blog_ml-pde/feed.json"><meta property="og:title" content="Búsqueda de Hiperparámetros"><meta property="og:site_name" content="Machine Learning & Partial Differential Equations"><meta property="og:description" content="Introducción La optimización de hiperparámetros es un componente esencial del entrenamiento de modelos de aprendizaje automático, como las redes neuronales multicapa (MLP). Los hiperparámetros son configuraciones del modelo que deben ser definidas antes del entrenamiento, y pueden influir significativamente en la capacidad del modelo para&hellip;"><meta property="og:url" content="https://jmmachadol.github.io/blog_ml-pde/busqueda-de-hiperparametros.html"><meta property="og:type" content="article"><link rel="stylesheet" href="https://jmmachadol.github.io/blog_ml-pde/assets/css/style.css?v=ad994e06fe4ccf9aa9bbf0f25d02eaf2"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://jmmachadol.github.io/blog_ml-pde/busqueda-de-hiperparametros.html"},"headline":"Búsqueda de Hiperparámetros","datePublished":"2023-06-28T18:51","dateModified":"2023-06-28T18:53","description":"Introducción La optimización de hiperparámetros es un componente esencial del entrenamiento de modelos de aprendizaje automático, como las redes neuronales multicapa (MLP). Los hiperparámetros son configuraciones del modelo que deben ser definidas antes del entrenamiento, y pueden influir significativamente en la capacidad del modelo para&hellip;","author":{"@type":"Person","name":"Jose Manuel Machado-Loaiza","url":"https://jmmachadol.github.io/blog_ml-pde/authors/jose-manuel-machado-loaiza/"},"publisher":{"@type":"Organization","name":"Jose Manuel Machado-Loaiza"}}</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://jmmachadol.github.io/blog_ml-pde/">Machine Learning &amp; Partial Differential Equations</a></header><main><article class="post"><div class="hero"><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2023-06-28T18:51">June 28, 2023</time></div><h1>Búsqueda de Hiperparámetros</h1><div class="post__meta post__meta--author"><a href="https://jmmachadol.github.io/blog_ml-pde/authors/jose-manuel-machado-loaiza/" class="feed__author">Jose Manuel Machado-Loaiza</a></div></div></header></div><div class="wrapper post__entry"><div id="content"><h2>Introducción</h2><p>La optimización de hiperparámetros es un componente esencial del entrenamiento de modelos de aprendizaje automático, como las redes neuronales multicapa (MLP). Los hiperparámetros son configuraciones del modelo que deben ser definidas antes del entrenamiento, y pueden influir significativamente en la capacidad del modelo para aprender de los datos y generalizar a nuevos ejemplos. En este entrada, exploramos y comparamos tres técnicas ampliamente utilizadas para optimizar hiperparámetros: Grid Search, Randomized Search y Bayesian Search.</p><h2>1. Grid Search</h2><p>El Grid Search, o búsqueda por cuadrícula, es una técnica de optimización de hiperparámetros que evalúa sistemáticamente todas las posibles combinaciones de hiperparámetros definidas. Aunque este enfoque exhaustivo puede ser costoso en términos de recursos computacionales, garantiza que se examine cada posible combinación de hiperparámetros, ofreciendo la posibilidad de encontrar el conjunto óptimo con un alto grado de certeza.</p><p>El Grid Search es particularmente útil cuando se dispone de suficientes recursos computacionales y el espacio de búsqueda es razonablemente pequeño. Sin embargo, su eficiencia disminuye a medida que aumenta el número de hiperparámetros y sus posibles valores, ya que el tiempo y los recursos necesarios crecen exponencialmente.</p><p> </p><h2>2. Randomized Search</h2><p>El Randomized Search, o búsqueda aleatoria, aborda algunas de las limitaciones del Grid Search al seleccionar aleatoriamente un subconjunto de combinaciones de hiperparámetros para evaluar. Esto puede resultar en una mayor eficiencia sin sacrificar demasiado la calidad de la solución encontrada.</p><p>A diferencia del Grid Search, que siempre probará las mismas combinaciones de hiperparámetros, el Randomized Search puede explorar una variedad más amplia de valores, lo que puede resultar beneficioso si el espacio de hiperparámetros es muy grande o si la distribución de los valores óptimos es desconocida.</p><p> </p><h2>3. Bayesian Search</h2><p>El Bayesian Search, o búsqueda bayesiana, es una técnica de optimización que utiliza métodos de estadística bayesiana para guiar la búsqueda de hiperparámetros. A diferencia de las técnicas anteriores, la búsqueda bayesiana utiliza los resultados de las evaluaciones anteriores para informar las elecciones de los futuros hiperparámetros a probar.</p><p>La principal ventaja de la búsqueda bayesiana es que puede encontrar conjuntos óptimos de hiperparámetros más rápidamente que el Grid Search y el Randomized Search, especialmente en espacios de alta dimensión. Sin embargo, puede ser más difícil de implementar y comprender que las técnicas más simples.</p><h2>Comparación y Conclusiones</h2><table><tbody><tr><th>Técnica</th><th>Ventajas</th><th>Desventajas</th></tr><tr><td>Grid Search</td><td>Exhaustivo, fácil de implementar y entender</td><td>Costoso computacionalmente, no escalable con muchas dimensiones</td></tr><tr><td>Randomized Search</td><td>Eficiente, escalable, exploración diversa de hiperparámetros</td><td>Posible falta de precisión, resultados pueden variar</td></tr><tr><td>Bayesian Search</td><td>Rápido en espacios de alta dimensión, aprende de iteraciones anteriores</td><td>Más complejo, implementación puede ser desafiante</td></tr></tbody></table><p>Optimizar hiperparámetros es vital en la formación de modelos de redes neuronales. Elegir los hiperparámetros correctos puede marcar la diferencia entre un modelo mediocre y uno excepcional. Aunque las técnicas avanzadas como la búsqueda bayesiana pueden ofrecer beneficios significativos en términos de eficiencia y precisión, las técnicas más simples como el Grid Search y el Randomized Search siguen siendo valiosas y pueden ser más apropiadas en determinadas situaciones. Como siempre, la elección de la técnica dependerá del problema específico, los datos disponibles y los recursos de cómputo.</p><h2>Referencias</h2><ul><li>Bergstra, J., &amp; Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13(Feb), 281-305.</li><li>Snoek, J., Larochelle, H., &amp; Adams, R. P. (2012). Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems (pp. 2951-2959).</li><li>Probst, P., Bischl, B., &amp; Boulesteix, A. L. (2019). Tunability: Importance of hyperparameters of machine learning algorithms. Journal of Machine Learning Research, 20(53), 1-32.</li></ul></div></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on June 28, 2023</p><div class="post__share"></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://jmmachadol.github.io/blog_ml-pde/authors/jose-manuel-machado-loaiza/" rel="author">Jose Manuel Machado-Loaiza</a></h3></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://jmmachadol.github.io/blog_ml-pde/assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="https://jmmachadol.github.io/blog_ml-pde/oe1.html" class="post__nav-link" rel="prev"><span>Previous</span> Teorema de Aproximación y Retropropagación</a></div></div></nav><div class="post__related related"><div class="wrapper"><h2 class="h5 related__title">You should also read:</h2><article class="related__item"><div class="feed__meta"><time datetime="2023-06-23T17:14" class="feed__date">June 23, 2023</time></div><h3 class="h1"><a href="https://jmmachadol.github.io/blog_ml-pde/parametros-e-hiperparametros-de-las-redes-neuronales-que-son-y-cuales-son-sus-diferencias.html">Parámetros e hiperparámetros de las redes neuronales</a></h3></article></div></div><div class="post__comments"><div class="wrapper"><div class="comments"><div class="comments-wrapper"><h2>Comments</h2><div id="disqus_thread"></div><noscript>Please enable JS to use the comments form.</noscript><script type="text/javascript">var disqus_config = function () {
							this.page.url = 'https://jmmachadol.github.io/blog_ml-pde/busqueda-de-hiperparametros.html';
							this.page.identifier = '9'; 
							this.language = 'en_GB';
						};
						
						
				var disqus_element_to_check = document.getElementById('disqus_thread');

				if ('IntersectionObserver' in window) {
					var iObserver = new IntersectionObserver(
						(entries, observer) => {
							entries.forEach(entry => {
								if (entry.intersectionRatio >= 0.1) {
									(function () {
										var d = document, s = d.createElement('script');
										s.src = 'https://'+('https-jmmachadol-github-io-blog-ml-pde').trim()+'.disqus.com/embed.js';
										s.setAttribute('data-timestamp', +new Date());
										(d.head || d.body).appendChild(s);
									})();
									observer.unobserve(entry.target);
								}
							});
						},
						{
							threshold: [0, 0.2, 0.5, 1]
						}
					);

					iObserver.observe(disqus_element_to_check);
				} else {
					(function () {
						var d = document, s = d.createElement('script');
						s.src = 'https://'+('https-jmmachadol-github-io-blog-ml-pde').trim()+'.disqus.com/embed.js';
						s.setAttribute('data-timestamp', +new Date());
						(d.head || d.body).appendChild(s);
					})();
				}</script><script type="text/javascript"></script></div></div></div></div></main><footer class="footer"><div class="footer__copyright"><p>Powered by Publii</p></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg><use xlink:href="https://jmmachadol.github.io/blog_ml-pde/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script defer="defer" src="https://jmmachadol.github.io/blog_ml-pde/assets/js/scripts.min.js?v=f47c11534595205f20935f0db2a62a85"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>