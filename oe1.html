<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Teorema de Aproximación y Retropropagación - Machine Learning &amp; Partial Differential Equations</title><meta name="description" content="Teorema de Aproximación Universal Un perceptrón multicapa (MLP) entrenado con el algoritmo de retropropagación puede ser visto como una herramienta efectiva para realizar un mapeo de entrada-salida no lineal. La relación de entrada-salida de una red define un mapeo desde un espacio de entrada euclidiano&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://jmmachadol.github.io/blog_ml-pde/oe1.html"><link rel="alternate" type="application/atom+xml" href="https://jmmachadol.github.io/blog_ml-pde/feed.xml"><link rel="alternate" type="application/json" href="https://jmmachadol.github.io/blog_ml-pde/feed.json"><meta property="og:title" content="Teorema de Aproximación y Retropropagación"><meta property="og:site_name" content="Machine Learning & Partial Differential Equations"><meta property="og:description" content="Teorema de Aproximación Universal Un perceptrón multicapa (MLP) entrenado con el algoritmo de retropropagación puede ser visto como una herramienta efectiva para realizar un mapeo de entrada-salida no lineal. La relación de entrada-salida de una red define un mapeo desde un espacio de entrada euclidiano&hellip;"><meta property="og:url" content="https://jmmachadol.github.io/blog_ml-pde/oe1.html"><meta property="og:type" content="article"><link rel="stylesheet" href="https://jmmachadol.github.io/blog_ml-pde/assets/css/style.css?v=ad994e06fe4ccf9aa9bbf0f25d02eaf2"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://jmmachadol.github.io/blog_ml-pde/oe1.html"},"headline":"Teorema de Aproximación y Retropropagación","datePublished":"2023-06-28T18:19","dateModified":"2023-06-28T18:52","description":"Teorema de Aproximación Universal Un perceptrón multicapa (MLP) entrenado con el algoritmo de retropropagación puede ser visto como una herramienta efectiva para realizar un mapeo de entrada-salida no lineal. La relación de entrada-salida de una red define un mapeo desde un espacio de entrada euclidiano&hellip;","author":{"@type":"Person","name":"Jose Manuel Machado-Loaiza","url":"https://jmmachadol.github.io/blog_ml-pde/authors/jose-manuel-machado-loaiza/"},"publisher":{"@type":"Organization","name":"Jose Manuel Machado-Loaiza"}}</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="https://jmmachadol.github.io/blog_ml-pde/">Machine Learning &amp; Partial Differential Equations</a></header><main><article class="post"><div class="hero"><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2023-06-28T18:19">June 28, 2023</time></div><h1>Teorema de Aproximación y Retropropagación</h1><div class="post__meta post__meta--author"><a href="https://jmmachadol.github.io/blog_ml-pde/authors/jose-manuel-machado-loaiza/" class="feed__author">Jose Manuel Machado-Loaiza</a></div></div></header></div><div class="wrapper post__entry"><div id="content"><h2>Teorema de Aproximación Universal</h2><p>Un perceptrón multicapa (MLP) entrenado con el algoritmo de retropropagación puede ser visto como una herramienta efectiva para realizar un mapeo de entrada-salida no lineal. La relación de entrada-salida de una red define un mapeo desde un espacio de entrada euclidiano <strong>m<sub>0</sub></strong>-dimensional a un espacio de salida euclidiano <strong>M</strong>-dimensional, que es infinitamente diferenciable en términos continuos si la función de activación también lo es (Cybenko, 1989).</p><figure class="post__image">Desde esta perspectiva de mapeo de entrada-salida, surge una pregunta fundamental: ¿cuál es el número mínimo de capas ocultas necesarias en un MLP para proporcionar una aproximación de cualquier mapeo continuo? Esta pregunta nos lleva al teorema de aproximación universal (Haykin, 2009). <img loading="lazy" src="https://jmmachadol.github.io/blog_ml-pde/media/posts/8/oe.png" alt="" width="1024" height="768" sizes="100vw" srcset="https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/oe-xs.png 300w, https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/oe-sm.png 480w, https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/oe-md.png 768w, https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/oe-lg.png 1024w, https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/oe-xl.png 1360w, https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/oe-2xl.png 1600w"></figure><p>El teorema de aproximación universal es un resultado clave en la teoría de las redes neuronales. Se establece que un MLP con una sola capa oculta puede aproximar cualquier función continua en un dominio compacto, siempre que la función de activación en la capa oculta sea una función no constante, acotada y continuamente diferenciable (Hornik, 1991). Este resultado fue extendido por Leshno et al. (1993), quienes demostraron que las redes neuronales con una función de activación no polinómica también pueden aproximar cualquier función continua.</p><p>Es importante resaltar que el teorema de aproximación universal no proporciona una guía sobre la cantidad de neuronas necesarias en la capa oculta para lograr una aproximación precisa, ni proporciona una estrategia para entrenar la red para alcanzar dicha aproximación. Estos son problemas que dependen de la función específica a ser aproximada y de la arquitectura de la red neuronal en cuestión.</p><h2>Implicaciones del Teorema de Aproximación Universal</h2><p>El teorema de aproximación universal tiene tres implicaciones importantes:</p><ol><li>Las redes neuronales multicapa son capaces, en principio, de aproximar cualquier función continua, lo que indica que pueden ser utilizadas en una amplia gama de aplicaciones.</li><li>Las funciones de activación no lineales son cruciales para que las redes neuronales sean capaces de aproximar efectivamente funciones complejas y no lineales.</li><li>Aunque el teorema de aproximación universal establece que una sola capa oculta es suficiente, en la práctica, las redes con múltiples capas ocultas pueden ser más eficientes en términos de la cantidad de neuronas requeridas y la facilidad de entrenamiento (Goodfellow et al., 2016).</li></ol><h2>Algoritmo de Retropropagación</h2><figure class="post__image">El algoritmo de retropropagación es una técnica utilizada para entrenar redes neuronales. Se basa en el método del gradiente descendente y la optimización de una función de pérdida con respecto a los pesos de la red. Consta de dos fases: propagación hacia adelante y propagación hacia atrás (Rumelhart et al., 1986). <img loading="lazy" src="https://jmmachadol.github.io/blog_ml-pde/media/posts/8/maxresdefault.jpg" alt="" width="1280" height="720" sizes="100vw" srcset="https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/maxresdefault-xs.jpg 300w, https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/maxresdefault-sm.jpg 480w, https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/maxresdefault-md.jpg 768w, https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/maxresdefault-lg.jpg 1024w, https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/maxresdefault-xl.jpg 1360w, https://jmmachadol.github.io/blog_ml-pde/media/posts/8/responsive/maxresdefault-2xl.jpg 1600w"></figure><p>Durante la propagación hacia adelante, la red recibe una entrada y genera una salida. La función de pérdida, que mide la discrepancia entre la salida de la red y la salida deseada, se evalúa utilizando la salida generada. En la fase de propagación hacia atrás, se calculan las derivadas parciales de la función de pérdida con respecto a cada peso y sesgo de la red utilizando la regla de la cadena. Estas derivadas parciales se utilizan para actualizar los pesos y sesgos de la red a través del método del gradiente descendente.</p><p>El proceso de retropropagación se repite hasta que se alcanza un criterio de convergencia definido, como un error de entrenamiento mínimo o un número máximo de épocas de entrenamiento.</p><h2>Importancia y Pertinencia para las Redes Neuronales</h2><p>Tanto el Teorema de Aproximación Universal como el Algoritmo de Retropropagación son de gran importancia y pertinencia para las redes neuronales por las siguientes razones:</p><ol><li>El Teorema de Aproximación Universal demuestra que las redes neuronales tienen la capacidad de aproximar cualquier función continua, lo que las convierte en herramientas poderosas para modelar y resolver problemas complejos en diversas áreas.</li><li>La retropropagación es esencial para el entrenamiento de redes neuronales, permitiendo que las redes aprendan a partir de datos y mejoren su rendimiento a medida que se ajustan los pesos y parámetros.</li><li>La combinación del Teorema de Aproximación Universal y la retropropagación proporciona una base teórica y práctica sólida para el diseño y entrenamiento de redes neuronales, lo que permite su aplicación en una amplia gama de problemas y tareas.</li></ol><h2>Referencias</h2><ul><li>Cybenko, G. (1989). Approximation by Superpositions of a Sigmoidal Function. Mathematics of Control, Signals, and Systems, 2, 303–314.</li><li>Haykin, S. (2009). Neural Networks and Learning Machines (3rd Edition). Pearson.</li><li>Hornik, K. (1991). Approximation Capabilities of Multilayer Feedforward Networks. Neural Networks, 4(2), 251–257.</li><li>Leshno, M., Lin, V., Pinkus, A., &amp; Schocken, S. (1993). Multilayer feedforward networks with a nonpolynomial activation function can approximate any function. Neural Networks, 6(6), 861–867.</li><li>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep Learning. MIT Press.</li><li>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323, 533–536.</li></ul></div></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on June 28, 2023</p><div class="post__share"></div><div class="post__bio bio"><div class="bio__info"><h3 class="bio__name"><a href="https://jmmachadol.github.io/blog_ml-pde/authors/jose-manuel-machado-loaiza/" rel="author">Jose Manuel Machado-Loaiza</a></h3></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://jmmachadol.github.io/blog_ml-pde/assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="https://jmmachadol.github.io/blog_ml-pde/perceptron-multicapa-una-mirada-a-las-redes-neuronales.html" class="post__nav-link" rel="prev"><span>Previous</span> Perceptrón Multicapa: una mirada a las redes neuronales</a></div><div class="post__nav-next"><a href="https://jmmachadol.github.io/blog_ml-pde/busqueda-de-hiperparametros.html" class="post__nav-link" rel="next"><span>Next</span> Búsqueda de Hiperparámetros </a><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="https://jmmachadol.github.io/blog_ml-pde/assets/svg/svg-map.svg#arrow-next"/></svg></div></div></nav><div class="post__comments"><div class="wrapper"><div class="comments"><div class="comments-wrapper"><h2>Comments</h2><div id="disqus_thread"></div><noscript>Please enable JS to use the comments form.</noscript><script type="text/javascript">var disqus_config = function () {
							this.page.url = 'https://jmmachadol.github.io/blog_ml-pde/oe1.html';
							this.page.identifier = '8'; 
							this.language = 'en_GB';
						};
						
						
				var disqus_element_to_check = document.getElementById('disqus_thread');

				if ('IntersectionObserver' in window) {
					var iObserver = new IntersectionObserver(
						(entries, observer) => {
							entries.forEach(entry => {
								if (entry.intersectionRatio >= 0.1) {
									(function () {
										var d = document, s = d.createElement('script');
										s.src = 'https://'+('https-jmmachadol-github-io-blog-ml-pde').trim()+'.disqus.com/embed.js';
										s.setAttribute('data-timestamp', +new Date());
										(d.head || d.body).appendChild(s);
									})();
									observer.unobserve(entry.target);
								}
							});
						},
						{
							threshold: [0, 0.2, 0.5, 1]
						}
					);

					iObserver.observe(disqus_element_to_check);
				} else {
					(function () {
						var d = document, s = d.createElement('script');
						s.src = 'https://'+('https-jmmachadol-github-io-blog-ml-pde').trim()+'.disqus.com/embed.js';
						s.setAttribute('data-timestamp', +new Date());
						(d.head || d.body).appendChild(s);
					})();
				}</script><script type="text/javascript"></script></div></div></div></div></main><footer class="footer"><div class="footer__copyright"><p>Powered by Publii</p></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg><use xlink:href="https://jmmachadol.github.io/blog_ml-pde/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script defer="defer" src="https://jmmachadol.github.io/blog_ml-pde/assets/js/scripts.min.js?v=f47c11534595205f20935f0db2a62a85"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>